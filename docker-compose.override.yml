# mount the dbt directory as a volume
services:
  scheduler:
    env_file:
      - compose.env
    command: bash -c "/usr/local/airflow/src/scripts/init_airflow.sh && airflow scheduler"
    networks:
      - airflow
    volumes:
      - ./dags/dbt/elt_pipeline_stocks:/usr/local/airflow/dags/dbt/elt_pipeline_stocks

  rustfs:
    image: rustfs/rustfs:1.0.0-alpha.69
    container_name: rustfs
    hostname: rustfs
    ports:
      - "9000:9000" # Puerto S3 API y Console
      - "9001:9001" # Puerto para Observabilidad/Administración
    volumes:
      - rustfs_data:/data # Persistencia de los objetos S3
      - ./logs/rustfs:/logs
    restart: always
    environment:
      RUSTFS_ROOT_USER: rustfsadmin
      RUSTFS_ROOT_PASSWORD: rustfsadmin
    networks:
      - airflow

  stocks_dwh_service:
    image: postgres:16
    container_name: stocks_dwh_postgres
    hostname: stocks_dwh_postgres
    ports:
      - "5433:5432"
    environment:
      # === DWH CREDENTIALS (PostgreSQL) ===
      POSTGRES_USER: postgres # <-- LOGIN Airflow in UI
      POSTGRES_PASSWORD: postgres # <-- PASSWORD Airflow in UI
      POSTGRES_DB: stocks_dwh # <-- SCHEMA/DATABASE NAME in Airflow UI
    volumes:
      - stocks_dwh_postgres_data:/var/lib/postgresql/data # Persistencia de los datos
    networks:
      - airflow

  dashboard:
    build: . # Reutiliza tu Dockerfile/requirements.txt de Astro
    container_name: stocks_dashboard
    env_file:
      - compose.env
    # IMPORTANTE: Streamlit no es parte de Airflow, hay que decirle qué ejecutar
    command: streamlit run ui/dashboard.py --server.port=8501 --server.address=0.0.0.0
    ports:
      - "8501:8501"
    networks:
      - airflow
    depends_on:
      - stocks_dwh_service

volumes:
  stocks_dwh_postgres_data:
  rustfs_data:

networks:
  airflow:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: "1500"
